from typing import List, AsyncGenerator
from .advanced_hybrid import AdvancedHybridRAG  # ‚úÖ Cambio: Importamos la versi√≥n avanzada
from srag.core import Chunk

class ModularRAG(AdvancedHybridRAG):
    """
    Estrategia RAG Modular (Actualizada con Advanced RAG).
    
    Orquesta m√∫ltiples m√≥dulos bas√°ndose en configuraci√≥n:
    
    Flow:
    1. [Adaptive] Clasificaci√≥n: ¬øNecesito RAG? -> Si no, responde directo.
    2. [HyDE] Transformaci√≥n (Opcional): Genera documento hipot√©tico.
    3. [Advanced Hybrid] Recuperaci√≥n: 
       - Multi-Query Expansion (si no se usa HyDE).
       - B√∫squeda Vectorial Masiva.
       - Cross-Encoder Reranking.
    4. [Generation] Respuesta final.
    """

    def __init__(self, llm, embedder, vector_store, 
                use_adaptive: bool = True,
                use_hyde: bool = False, 
                use_hybrid: bool = True,
                reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"): # ‚úÖ Nuevo param
        
        # Inicializamos la clase padre (AdvancedHybridRAG) que carga el reranker
        super().__init__(llm, embedder, vector_store, reranker_model=reranker_model)
        
        self.use_adaptive = use_adaptive
        self.use_hyde = use_hyde
        self.use_hybrid = use_hybrid

    async def _classify_query(self, query: str) -> bool:
        """M√≥dulo Adaptive: Decide si buscar o no."""
        # Mantenemos la l√≥gica original que funcionaba bien
        print("ü§î [Modular] Analizando intenci√≥n de la pregunta...")
        
        prompt = f"""Eres un clasificador de consultas para un sistema RAG. Tu √∫nica tarea es decidir si la pregunta del usuario requiere buscar informaci√≥n en la base de datos documental.

        Criterios para BUSCAR:
        - Preguntas sobre documentos, archivos, textos espec√≠ficos.
        - Preguntas t√©cnicas, definiciones, res√∫menes.
        - Preguntas sobre "el texto", "el documento", "hitos", "arquitectura".

        Criterios para CHATEAR:
        - Saludos (Hola, buenos d√≠as).
        - Preguntas personales al bot (¬øQui√©n eres?, ¬øEst√°s bien?).
        - Preguntas generales fuera del contexto (¬øCu√°nto es 2+2?).

        Pregunta: "{query}"

        Responde SOLO con una palabra: "BUSCAR" o "CHATEAR"."""
        
        resp = await self.llm.generate(prompt)
        clean_resp = resp.strip().upper()
        should_search = "BUSCAR" in clean_resp
        
        print(f"   -> Router LLM dijo: '{clean_resp}'")
        return should_search

    async def _generate_hyde_doc(self, query: str) -> str:
        """M√≥dulo HyDE: Genera documento hipot√©tico."""
        print("üëª [Modular] Generando alucinaci√≥n hipot√©tica (HyDE)...")
        prompt = f"""Escribe un breve p√°rrafo t√©cnico que responda idealmente a: "{query}". Inventa los datos si es necesario."""
        fake_doc = await self.llm.generate(prompt)
        return fake_doc

    async def retrieve(self, query: str, k: int = 4, **kwargs) -> List[Chunk]:
        search_query = query

        # 1. M√≥dulo HyDE (Query Transformation)
        # Nota: Si activamos HyDE, la "Expansion" del AdvancedHybrid se har√° sobre
        # el documento hipot√©tico, lo cual puede ser muy potente o redundante.
        if self.use_hyde:
            fake_doc = await self._generate_hyde_doc(query)
            search_query = fake_doc
        
        # 2. M√≥dulo de Recuperaci√≥n (Advanced Hybrid o Simple)
        if self.use_hybrid:
            # ‚úÖ Llama al retrieve de AdvancedHybridRAG (Expansion + Vector + Rerank)
            return await super().retrieve(search_query, k=k)
        else:
            # Fallback a b√∫squeda vectorial simple (sin rerank, sin expansion)
            print(f"üîç [Modular] B√∫squeda Vectorial Simple: '{search_query[:50]}...'")
            q_vec = await self.embedder.embed_query(search_query)
            return await self.vector_store.search(q_vec, k=k)

    async def stream(self, query: str, k: int = 4, **kwargs) -> AsyncGenerator[str, None]:
        # El flujo principal se mantiene igual, la magia ocurre dentro de retrieve()
        
        # 1. M√≥dulo Adaptive
        if self.use_adaptive:
            needs_rag = await self._classify_query(query)
            if not needs_rag:
                print("‚ö° [Modular] Modo Chat Directo")
                async for token in self.llm.stream(query):
                    yield token
                return

        # 2. Recuperaci√≥n Avanzada
        chunks = await self.retrieve(query, k=k)
        
        if not chunks:
            yield "No encontr√© informaci√≥n relevante."
            return

        # 3. Generaci√≥n
        context = self._build_context(chunks)
        print("ü§ñ [Modular] Generando respuesta final...")
        
        prompt = f"""Usa el siguiente contexto recuperado y reordenado para responder.
        
CONTEXTO:
{context}

PREGUNTA ORIGINAL: {query}
RESPUESTA:"""
        
        async for token in self.llm.stream(prompt):
            yield token