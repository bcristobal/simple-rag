from typing import List, AsyncGenerator, Set
from .base import BaseRAGStrategy
from srag.core import Chunk
from .hybrid import HybridRAG  # Reutilizamos la l√≥gica de Hybrid y RRF

class ModularRAG(HybridRAG):
    """
    Estrategia RAG Modular (State of the Art).
    Orquesta m√∫ltiples m√≥dulos bas√°ndose en configuraci√≥n:
    
    Flow:
    1. [Adaptive] Clasificaci√≥n: ¬øNecesito RAG? -> Si no, responde directo.
    2. [HyDE] Transformaci√≥n: ¬øGenero documento hipot√©tico para mejorar b√∫squeda?
    3. [Hybrid] Recuperaci√≥n: Vectores + Keywords + RRF Fusion.
    4. [Generation] Respuesta final.
    """

    def __init__(self, llm, embedder, vector_store, 
                use_adaptive: bool = True,
                use_hyde: bool = False, 
                use_hybrid: bool = True):
        super().__init__(llm, embedder, vector_store)
        self.use_adaptive = use_adaptive
        self.use_hyde = use_hyde
        self.use_hybrid = use_hybrid

    async def _classify_query(self, query: str) -> bool:
        """M√≥dulo Adaptive: Decide si buscar o no (Versi√≥n Robusta)."""
        print("ü§î [Modular] Analizando intenci√≥n de la pregunta...")
        
        # PROMPT MEJORADO (Igual que en AdaptiveRAG que funcion√≥ bien)
        prompt = f"""Eres un clasificador de consultas para un sistema RAG. Tu √∫nica tarea es decidir si la pregunta del usuario requiere buscar informaci√≥n en la base de datos documental.

        Criterios para BUSCAR:
        - Preguntas sobre documentos, archivos, textos espec√≠ficos.
        - Preguntas t√©cnicas, definiciones, res√∫menes.
        - Preguntas sobre "el texto", "el documento", "hitos", "arquitectura".

        Criterios para CHATEAR:
        - Saludos (Hola, buenos d√≠as).
        - Preguntas personales al bot (¬øQui√©n eres?, ¬øEst√°s bien?).
        - Preguntas generales fuera del contexto (¬øCu√°nto es 2+2?).

        Pregunta: "{query}"

        Responde SOLO con una palabra: "BUSCAR" o "CHATEAR"."""
        
        # Usamos generate para obtener la decisi√≥n
        resp = await self.llm.generate(prompt)
        clean_resp = resp.strip().upper()
        
        # L√≥gica de decisi√≥n m√°s permisiva
        should_search = "BUSCAR" in clean_resp
        
        print(f"   -> Router LLM dijo: '{clean_resp}'")
        print(f"   -> Decisi√≥n final: {'‚úÖ Requiere RAG' if should_search else '‚ö° Conversaci√≥n directa'}")
        
        return should_search

    async def _generate_hyde_doc(self, query: str) -> str:
        """M√≥dulo HyDE: Genera documento hipot√©tico."""
        print("üëª [Modular] Generando alucinaci√≥n hipot√©tica (HyDE)...")
        prompt = f"""Escribe un breve p√°rrafo t√©cnico que responda idealmente a: "{query}". Inventa los datos si es necesario."""
        fake_doc = await self.llm.generate(prompt)
        return fake_doc

    async def retrieve(self, query: str, k: int = 4, **kwargs) -> List[Chunk]:
        vector_search_text = query

        # 1. M√≥dulo HyDE: Transformamos SOLO la query vectorial
        if self.use_hyde:
            fake_doc = await self._generate_hyde_doc(query)
            vector_search_text = fake_doc  # La alucinaci√≥n
            print("   üëª HyDE: Usando documento hipot√©tico para b√∫squeda vectorial.")
        
        # 2. M√≥dulo H√≠brido
        if self.use_hybrid:
            # MAGIA: Pasamos la alucinaci√≥n para el vector, 
            # pero HybridRAG usar√° 'query' (original) para las palabras clave.
            return await super().retrieve(
                query=query,          # Original para Keywords
                vector_query=vector_search_text, # Alucinaci√≥n para Vectores
                k=k
            )
        else:
            # Fallback simple
            q_vec = await self.embedder.embed_query(vector_search_text)
            return await self.vector_store.search(q_vec, k=k)

    async def stream(self, query: str, k: int = 4, **kwargs) -> AsyncGenerator[str, None]:
        # 1. M√≥dulo Adaptive (Pre-retrieval)
        if self.use_adaptive:
            needs_rag = await self._classify_query(query)
            if not needs_rag:
                print("‚ö° [Modular] Modo Chat Directo (Fast path)")
                async for token in self.llm.stream(query):
                    yield token
                return

        # 2. Recuperaci√≥n (Incluye HyDE y Hybrid si est√°n activos)
        chunks = await self.retrieve(query, k=k)
        
        if not chunks:
            yield "No encontr√© informaci√≥n relevante."
            return

        # 3. Generaci√≥n Final
        context = self._build_context(chunks)
        print("ü§ñ [Modular] Generando respuesta final...")
        
        prompt = f"""Usa el contexto proporcionado para responder.
        
CONTEXTO:
{context}

PREGUNTA ORIGINAL: {query}
RESPUESTA:"""
        
        async for token in self.llm.stream(prompt):
            yield token